@inproceedings{Achmamad2022FewShotUNet,
  abstract = {This work combines meta-learning with a U‑Net backbone to enable accurate brain‑tumor MRI segmentation from only one or a few annotated examples. Episodic training teaches the network to adapt quickly to new tasks, achieving markedly higher Dice scores than conventional fine‑tuning under extreme data scarcity.},
  author   = {Achmamad, Afif and Ghazouani, Fatma and Ruan, Su},
  doi      = {10.1109/ICSP56322.2022.9965315},
  booktitle = {IEEE International Conference on Signal Processing (ICSP)},
  keywords = {type:method, few_shot_learning, meta_learning, U-Net, MRI, brain_tumor_segmentation},
  publisher = {IEEE},
  title    = {Few‑Shot Learning for Brain Tumor Segmentation from MRI Images},
  url      = {https://ieeexplore.ieee.org/document/9965315},
  year     = {2022}
}

@inproceedings{Cicek20163DUNet,
  abstract = {3D U‑Net extends the original 2D U‑Net to volumetric data by employing 3D convolutions. It learns dense volumetric segmentations from sparsely annotated MRI, delivering state‑of‑the‑art accuracy on tasks such as brain tumor segmentation by leveraging full 3D context.},
  author   = {Çiçek, Özgün and Abdulkadir, Ahmed and Lienkamp, Sara S. and Brox, Thomas and Ronneberger, Olaf},
  doi      = {10.1007/978-3-319-46723-8_49},
  booktitle = {Medical Image Computing and Computer Assisted Intervention (MICCAI)},
  keywords = {type:method, 3D_convolution, U-Net, volumetric_segmentation, MRI},
  publisher = {Springer},
  volume    = {9901},
  title     = {3D U‑Net: Learning Dense Volumetric Segmentation from Sparse Annotation},
  url       = {https://link.springer.com/chapter/10.1007/978-3-319-46723-8_49},
  year      = {2016}
}

@article{Dechesne2021BayesianUNet,
  abstract = {A Bayesian variant of U‑Net employing Monte‑Carlo dropout at inference to quantify voxel‑wise epistemic uncertainty in medical image segmentation. The method achieves competitive accuracy while providing reliable uncertainty maps that correlate with ambiguous regions in MRI scans.},
  author   = {Dechesne, C{\'e}dric and Kang, MiKyung and Lalande, Alexandre and Lecerf, Marc},
  doi      = {10.3390/rs13193836},
  journal  = {Remote Sensing},
  keywords = {type:method, bayesian_deep_learning, uncertainty_estimation, U-Net, MRI},
  number   = {19},
  publisher = {MDPI},
  volume   = {13},
  title    = {Bayesian U‑Net: Estimating Uncertainty in Semantic Segmentation of Medical Images},
  url      = {https://www.mdpi.com/2072-4292/13/19/3836},
  year     = {2021}
}



@article{Oktay2018AttentionUNet,
  abstract = {Attention U‑Net introduces trainable attention gates that suppress irrelevant regions in encoder–decoder skip connections, enabling the network to focus on tumor tissue. The mechanism improves sensitivity to small structures and is lightweight to integrate into U‑Net variants.},
  author   = {Oktay, Ozan and Schlemper, Jo and Le Folgoc, Lo{\"i}c and others},
  doi      = {10.48550/arXiv.1804.03999},
  journal  = {arXiv preprint arXiv:1804.03999},
  keywords = {type:method, attention_mechanism, U-Net, medical_segmentation},
  title    = {Attention U‑Net: Learning Where to Look for the Pancreas (and Other Organs)},
  url      = {https://arxiv.org/abs/1804.03999},
  year     = {2018}
}



@article{Zhao2022MMUNet,
  abstract = {MM‑UNet employs a separate encoder per MRI modality with hybrid attention to fuse modality‑specific features in the decoder, achieving superior brain‑tumor segmentation accuracy versus single‑encoder baselines on BraTS 2020.},
  author   = {Zhao, Jin and Ma, Hua and Shao, Xin and Jia, Wenyan and Zhao, Yifan and Yuan, Haowen},
  doi      = {10.3389/fonc.2022.950706},
  journal  = {Frontiers in Oncology},
  keywords = {type:method, multimodal_fusion, attention, U-Net, brain_tumor},
  number   = {},
  publisher = {Frontiers},
  volume   = {12},
  title    = {MM‑UNet: A Multimodality Feature Fusion Network for Brain Tumor Segmentation},
  url      = {https://www.frontiersin.org/articles/10.3389/fonc.2022.950706},
  year     = {2022}
}

@inproceedings{Yan2024MetaTransfer,
  abstract = {Proposes a meta‑transfer learning framework enabling rapid adaptation of a glioma‑trained segmentation model to novel tumor types with only a few labeled scans, boosting Dice scores by ~30\% over standard fine‑tuning.},
  author   = {Yan, Shuai and Liu, Shuang and Di Ieva, Antonio and Marek, Jiří and Faes, Luca},
  doi      = {10.1007/978-3-031-64892-2_13},
  booktitle = {Advances in Experimental Medicine and Biology},
  keywords = {type:method, meta_learning, transfer_learning, few_shot, brain_tumor_segmentation},
  publisher = {Springer},
  volume    = {1462},
  title     = {Meta‑Transfer Learning for Brain Tumor Segmentation: Within and Beyond Glioma},
  url       = {https://link.springer.com/chapter/10.1007/978-3-031-64892-2_13},
  year      = {2024}
}

@article{Maddirala2023PrototypeOneShot,
  abstract = {Introduces a foreground‑prototype based one‑shot brain‑tumor segmentation framework that achieves 83\% Dice score on BraTS 2021 using only a single annotated example, highlighting the potential of metric‑learning for extreme data scarcity.},
  author   = {Maddirala, Visakh K. and Balasundaram, Anand and Kavitha, M. Sini and Pratheepan, Yasitha},
  doi      = {10.3390/diagnostics13071282},
  journal  = {Diagnostics},
  keywords = {type:method, one_shot_learning, prototype_learning, brain_tumor_segmentation},
  number   = {7},
  publisher = {MDPI},
  volume   = {13},
  title    = {Foreground Prototype‑Based One‑Shot Segmentation of Brain Tumors},
  url      = {https://www.mdpi.com/2075-4418/13/7/1282},
  year     = {2023}
}
@article{Isensee2021nnUNet,
  abstract = {Presents nnU-Net, a self-configuring segmentation framework that adapts automatically to any biomedical segmentation task without manual tuning, achieving state-of-the-art results.},
  author   = {Isensee, Fabian and Jaeger, Paul and Kohl, Simon and Petersen, Jens and Maier-Hein, Klaus H.},
  doi      = {10.1038/s41592-020-01008-z},
  journal  = {Nature Methods},
  keywords = {type:framework, nnunet, autoML, biomedical_segmentation},
  number   = {2},
  publisher = {Nature Publishing Group},
  volume   = {18},
  title    = {nnU-Net: A Self-Configuring Method for Deep Learning-Based Biomedical Image Segmentation},
  url      = {https://doi.org/10.1038/s41592-020-01008-z},
  year     = {2021}
}
@inproceedings{Wang2021TransBTS,
  abstract  = {Proposes TransBTS, a transformer-based brain tumor segmentation model that integrates convolutional and attention modules to capture long-range dependencies in multimodal MRI.},
  author    = {Wang, Wentao and Chen, Chen and Ding, Meng and Li, Jing and Yu, Hong and Zha, Shengxiang},
  doi       = {10.1007/978-3-030-87193-2_11},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  keywords  = {type:architecture, transformer, multimodal, brain_tumor_segmentation},
  pages     = {109--119},
  publisher = {Springer},
  volume    = {12901},
  title     = {TransBTS: Multimodal Brain Tumor Segmentation Using Transformer},
  url       = {https://doi.org/10.1007/978-3-030-87193-2_11},
  year      = {2021}
}
@inproceedings{Milletari2016VNet,
  abstract  = {Presents V-Net, a fully convolutional neural network designed for 3D medical image segmentation with Dice loss, demonstrating strong performance on prostate MRI.},
  author    = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  doi       = {10.1109/3DV.2016.79},
  booktitle = {Proceedings of the Fourth International Conference on 3D Vision (3DV)},
  keywords  = {type:architecture, vnet, dice_loss, 3d_segmentation},
  pages     = {565--571},
  publisher = {IEEE},
  title     = {V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation},
  url       = {https://doi.org/10.1109/3DV.2016.79},
  year      = {2016}
}


